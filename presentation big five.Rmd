---
title: "Presentation Personality from Vlogs"
author: "Yina Tsai, Natalie Glomsda, Balint Bojko, Yeachan Park, Andreas Hopf"
date: "18 September 2018"
output:
  pdf_document: default
  html_document: default
---



#General Timeline

###   1. Data Wrangling

###   2. Literature review

   - Audiovisual features
   - Transcript based features

###   3. Initial model selection 

###   4. Feature engineering 

###   5. Final model selection






#Data Wrangling

```{r}
options(warn = -1)

library(tidyverse)

" 
Read in the CSV file
"
personality = as.tibble(read.csv("YouTube-Personality-Personality_impression_scores_train.csv", 
                                 encoding = "UTF-8",
                                 sep = " "))

gender = as.tibble(read.csv("YouTube-Personality-gender.csv",
                            encoding = "UTF-8",
                            sep = " "))

audiovisual = as.tibble(read.csv("YouTube-Personality-audiovisual_features.csv",
                                 encoding = "UTF-8",
                                 sep = " "))

transcripts = as.tibble(read.csv("transcripts.csv",
                                 encoding = "UTF-8",
                                 sep = " "))

transcripts$index = as.character(transcripts$index)

transcripts =
  transcripts %>%
  rename(vlogId = index,
         transcript = X0) %>%
  mutate(vlogId = substr(vlogId, 1, nchar(vlogId)-4))

personality$vlogId = as.character(personality$vlogId)
gender$vlogId = as.character(gender$vlogId)
audiovisual$vlogId = as.character(audiovisual$vlogId)

training_data = 
  personality %>%
  left_join(gender, on = c("vlogID" = "vlogID")) %>%
  left_join(audiovisual, on = c("vlogID" = "vlogID")) %>%
  left_join(transcripts, on = c("clogID" = "vlogID"))

```

#Literature review

J.-I. Biel and D. Gatica-Perez, "The YouTube Lens: Crowdsourced Personality Impressions
and Audiovisual Analysis of Vlogs" in IEEE Transactions on Multimedia , Vol. 15, No. 1,
pp. 41-55, Jan. 2013.

Sarkar, C., Bhatia, S., Agarwal, A., & Li, J. (2014, November). Feature analysis for computational personality recognition using youtube personality data set. In Proceedings of the 2014 ACM Multi Media on Workshop on Computational Personality Recognition (pp. 11-14). ACM.

J-I. Biel, O. Aran, and D. Gatica-Perez, "You Are Known by How You Vlog: Personality 
Impressions and Nonverbal Behavior in YouTube" In Proc. AAAI Int. Conf. . on Weblogs
and Social Media (ICWSM) , Barcelona, July 2011.


Sarkar, C., Bhatia, S., Agarwal, A., & Li, J. (2014, November). Feature analysis for computational personality recognition using youtube personality data set. In Proceedings of the 2014 ACM Multi Media on Workshop on Computational Personality Recognition (pp. 11-14). ACM.

Yarkoni, T. (2010). Personality in 100,000 words: A large-scale analysis of personality and word use among bloggers. Journal of research in personality, 44(3), 363-373.

Jockers, M. (2017). Package 'syuzhet'. URL: https://cran. r-project. org/web/packages/syuzhet.

```{r}
#predictors based on transcripts

#establish wordcount function
wordcount <- function(vector_string,path_txt_file, sep)
{
  library(stringr)
  wordlist <- as.vector(colnames(read.csv(path_txt_file,sep=sep)))
  
  count_vector <- vector(length = length(vector_string))
  
  for (j in 1:length(vector_string))
  {
    sum <- 0
    for (i in 1:length(wordlist))
    {
      sum <- sum + str_count(vector_string[j],wordlist[i])
    }
    count_vector[j] <- sum
  }
  return(count_vector)
}

#relative frequency of words associated with Big Five (input library can be modified)
training_data$words_Extraversion <- wordcount(training_data$transcript, "words E.txt", sep = ",") / str_count(training_data$transcript)
training_data$words_Neuroticism <- wordcount(training_data$transcript, "words N.txt", sep = ",") / str_count(training_data$transcript)
training_data$words_Openness <- wordcount(training_data$transcript, "words O.txt", sep = ",") / str_count(training_data$transcript)
training_data$words_Agreeableness <- wordcount(training_data$transcript, "words A.txt", sep = ",") / str_count(training_data$transcript)
training_data$words_Conscientiousness <- wordcount(training_data$transcript, "words C.txt", sep = ",") / str_count(training_data$transcript)

training_data$words_nExtraversion <- wordcount(training_data$transcript, "words negative E.txt", sep = ",") / str_count(training_data$transcript)
training_data$words_nNeuroticism <- wordcount(training_data$transcript, "words negative N.txt", sep = ",") / str_count(training_data$transcript)
training_data$words_nOpenness <- wordcount(training_data$transcript, "words negative O.txt", sep = ",") / str_count(training_data$transcript)
training_data$words_nAgreeableness <- wordcount(training_data$transcript, "words negative A.txt", sep = ",") / str_count(training_data$transcript)
training_data$words_nConscientiousness <- wordcount(training_data$transcript, "words negative C.txt", sep = ",") / str_count(training_data$transcript)


#sentiment scores from syuzhet package - positive = positive emotions, negative = negative emotions

library(syuzhet)

training_data$sentiment <- get_sentiment(as.vector(training_data$transcript), method = "afinn")
#method is interchangable within the syuzhet package; maybe also adjust for transcript length


#add word count for sentiment categories from nrc - scores for each transcript ranging from 0+
nrc_sentiments = get_nrc_sentiment(as.vector(training_data$transcript))

training_data$anger <- nrc_sentiments[,1]
training_data$anticipation <- nrc_sentiments[,2]
training_data$disgust <- nrc_sentiments[,3]
training_data$fear <- nrc_sentiments[,4]
training_data$joy <- nrc_sentiments[,5]
training_data$sadness <- nrc_sentiments[,6]
training_data$surprise <- nrc_sentiments[,7]
training_data$trust <- nrc_sentiments[,8]

training_data
```


#Initial model selection

##1. Build baseline model from all possible predictors
```{r}
baseline <- lm(Extr ~ ., data = training_data[,c(2,7:32,34:52)])
summary(baseline)
```

##2. Check for multicollinearity
```{r}
library(car)

vif(baseline)[vif(baseline) > 10]
```

##3. Build new model 
```{r}
model <- lm(Extr ~ . - words_Conscientiousness - mean.conf.pitch - mean.spec.entropy - mean.num.apeak, data = training_data[,c(2,7:32,34:52)])
summary(model)
```

##4. Use backward to "shrink" the model
```{r}
library(olsrr)

selection <- ols_step_backward_p(model)
plot(selection)

model_new <- lm(Extr ~ . - words_Conscientiousness - mean.conf.pitch - mean.spec.entropy - mean.num.apeak - disgust - words_nConscientiousness - anger - sd.energy - words_nExtraversion - anticipation - words_nOpenness - words_nNeuroticism - sd.num.apeak - hogv.cogR - words_nAgreeableness - mean.val.apeak - hogv.cogC - sd.pitch - avg.voiced.seg - sd.conf.pitch - words_Openness - surprise - sadness - hogv.median, data = training_data[,c(2,7:32,34:52)])

summary(model_new)

```






































